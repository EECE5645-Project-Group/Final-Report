The dataset used within this project is the Diabetic Retinopathy Detection Dataset provided by Kaggle \cite{noauthor_diabetic_nodate}. The dataset contains over 35,000 high resolution retina images from both eyes, captured under various imaging conditions using different camera models. All images are labeled by a medical professional with a retinopathy progression score on a 0 to 4 scale: 0 (No DR), 1 (Mild), 2 (Moderate), 3 (Severe), and 4 (Proliferate DR). The dataset comes structured in training, testing, and sample files, with a separate file delineating the training labels. We are focusing on identifying severe retinopathy, so we first reorganize the dataset into a two class system: images with progression score of zero identifying no retinopathy, and images with a progression score of three or four representing severe retinopathy. Images with a progression score of one or two will be discarded as they represent only moderate retinopathy progression.

The model we employ for classification is VGG16 \cite{simonyan_very_2015}, a popular pretrained CNN consisting of 16 convolutional and fully connected layers. Throughout its network, it uses 3x3 convolutional filtering and max-pooling layers \cite{graham_fractional_2015}, reducing the computational load in the model by reducing its number of required parameters. VGG16 was originally trained on ImageNet, a large image database for generalized images with over 1000 identifiable categories \cite{deng_imagenet_2009}. This allows for it to be applicable as a transfer learning model to a wide array of tasks that require identifying small features from an image \cite{deng_imagenet_2009}.

Images in the dataset come in varying sizes and coloring, due to them all begin taken under differing conditions. To account for this, we normalize the images in the dataset to a consistent size required by VGG16, and normalize them to have a zero mean and standard deviation of 1. The specific values chosen to find the zero mean and unit variance are identical to the ones originally used to preprocess images for VGG16, allowing for it to classify images in a similar environment it was trained in.

The PyTorch package was used to train the model \cite{paszke_pytorch_2019}. We allowed all model layers to be retrainable and added an additional 1024 unit dense layer with ReLU activation, a 50\% dropout layer, followed by a linear layer with a single node and sigmoid activation (due to the nature of binary classification) \cite{narayan_generalized_1997}. Binary cross entropy loss and the Adam optimizer with a learning rate of 0.001 was used to train the model \cite{kingma_adam_2017, zhang_generalized_2018}. Finally, the model was trained for 50 epochs with a batch size of 32.

To capture the speedup of parallelization, the model was trained twice, once on an NVIDIA T4 GPU and once on a CPU using the discovery cluster. The training/validation loss, training/validation AUC-ROC, and time-per-epoch were recorded in both cases \cite{davis_relationship_2006}.