Most easily parallelizable tasks and algorithms (consisting of commutative and associative operations when applied to an intended dataset, irrespective of the order in which the data is stored) can be accelerated in hardware \cite{pottenger_role_1998}. A straightforward way to accelerate a program that supports parallelism is to utilize multiple cores or threads on a system's CPU. Though introducing parallelism in this way can improve the performance of various parallelizable tasks, modern consumer and enthusiast-grade CPUs contain an upper threshold of 96 cores and 196 threads, as recently presented by AMD's unveiling of their flagship Threadripper Pro 7995WX \cite{noauthor_amd_nodate}. While high core and thread-count CPUs present the opportunity to accelerate parallelizable tasks, specialized hardware (GPUs) built explicitly with parallelism in mind execute parallel programs more efficiently \cite{pottenger_role_1998}. In the case of image processing tasks, specifically CNNs, as they relate to this paper, matrix-based operations such as convolutions can be efficiently executed on GPUs thanks to the presence of dedicated matrix multiplication units (MMUs) and streamlined memory to processor pipelines \cite{fatahalian_understanding_2004}. Additionally, modern GPUs contain thousands of processing cores (albeit these cores have a much lower clock frequency when compared to modern CPUs), with the NVIDIA T4, the GPU used to train the models presented in this paper, containing 2560 shading cores and 360 tensor cores optimized for operations on tensor objects \cite{emily_apsey_tesla_2019}. Given their optimized matrix operation pipelines and drastically increased core counts, GPUs have become the go-to device on which deep learning algorithms are trained \cite{steinkraus_using_2005}.
